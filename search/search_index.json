{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAGBOT CLI","text":"<p>By University of Cantabria.</p> <pre><code>@author Rub\u00e9n Mart\u00ednez Amodia\n@email  ruben.martinezam@alumnos.unican.es\n@supervisor Cristina Tirnauca\n@supervisor Marta Zorrilla\n</code></pre> <p><code>RAGBOT</code> is a command-line tool designed for running and evaluating Retrieval-Augmented Generation (RAG) chatbots. Developed as part of a bachelor thesis project, it supports the creation of intelligent virtual assistants through flexible experimentation and principled evaluation.</p> <p>Built atop the powerful <code>LangChain</code> framework and integrated with <code>LangSmith</code> for seamless experiment tracking and logging, <code>RAGBOT</code> offers a complete workflow for iterating on RAG pipelines.</p> <p>The tool provides a structured and extensible framework that enables:</p> <ul> <li>Interactive development and testing of RAG chatbot configurations.</li> <li>Customizable control over fine-tuning parameters, LLM selection, embedding strategies, and retriever behavior.</li> <li>Lightweight, modular evaluation inspired by the <code>RAGAS</code> framework\u2014yet more affordable and adaptable to diverse experimentation needs.</li> </ul> <p>Evaluation in <code>RAGBOT</code> combines multiple methodologies, including:</p> <ul> <li>LLM-as-a-judge techniques for subjective assessments,</li> <li>Embedding-based similarity scoring,</li> <li>And additional custom metrics for deeper analysis.</li> </ul> <p>Whether you're optimizing a chatbot's responses or conducting rigorous academic evaluations, <code>RAGBOT</code> offers the tools to explore, refine, and understand the behavior of modern RAG systems from the command line.</p> <p>\ud83e\uddea See the CLI Overview to get started. \ud83d\udcda Dive into the API Reference for detailed documentation. \ud83d\udcc2 To explore the source code, use the GitHub link in the upper-right corner of this site.</p>"},{"location":"cli/","title":"CLI Overview","text":"<p>RAGBOT provides a command-line interface to interact with and evaluate Retrieval-Augmented Generation (RAG) chatbots. The CLI supports two main functionalities:</p> <ol> <li>Chat with a RAG-powered virtual assistant using configurable parameters.</li> <li>Evaluate the performance of different configurations or models using a dataset and metrics.</li> </ol>"},{"location":"cli/#basic-usage","title":"Basic Usage","text":"<p>To run any CLI command, use:</p> <pre><code>poetry run python -m ragbot.cli &lt;command&gt; [options]\n</code></pre> <p>You can display help information by appending <code>-h</code> or <code>--help</code> to any command.</p>"},{"location":"cli/#available-commands","title":"Available Commands","text":""},{"location":"cli/#chat","title":"<code>chat</code>","text":"<p>Launches an interactive RAG chatbot session.</p>"},{"location":"cli/#usage","title":"Usage","text":"<pre><code>poetry run python -m ragbot.cli chat -p &lt;project&gt; [options]\n</code></pre>"},{"location":"cli/#required-argument","title":"Required argument","text":"<ul> <li><code>-p, --proj</code>: The name of the project to run.</li> </ul>"},{"location":"cli/#optional-arguments","title":"Optional arguments","text":"<ul> <li><code>--llm-provider</code>: LLM provider name (e.g., <code>google</code>).</li> <li><code>--llm</code>: LLM model name (e.g., <code>gemini-2.0-flash</code>).</li> <li><code>--temperature</code>: Temperature setting for sampling.</li> <li><code>--top-p</code>: Nucleus sampling value.</li> <li><code>--top-k</code>: Number of top tokens to consider.</li> <li><code>--emb-provider</code>: Embeddings provider (e.g., <code>google</code>).</li> <li><code>--emb-model</code>: Embedding model name (e.g., <code>models/embedding-001</code>).</li> <li><code>--chunk-size</code>: Chunk size for the text splitter.</li> <li><code>--chunk-overlap</code>: Overlap between chunks.</li> <li><code>--search-type</code>: Type of retrieval search.</li> <li><code>--k</code>: Number of documents to retrieve.</li> </ul> <p>This command lets you test chatbot behavior interactively while tweaking RAG parameters.</p>"},{"location":"cli/#evaluate","title":"<code>evaluate</code>","text":"<p>Evaluates a RAG chatbot setup using a given dataset and configuration.</p>"},{"location":"cli/#usage_1","title":"Usage","text":"<pre><code>poetry run python -m ragbot.cli evaluate -p &lt;project&gt; [options]\n</code></pre>"},{"location":"cli/#required-argument_1","title":"Required argument","text":"<ul> <li><code>-p, --proj</code>: The name of the project to evaluate.</li> </ul>"},{"location":"cli/#optional-arguments_1","title":"Optional arguments","text":"<ul> <li><code>--config-path</code>: Path to a JSON configuration file defining the evaluation setup.</li> <li><code>--dataset-name</code>: Name of the LangSmith dataset to evaluate on.</li> </ul> <p>This command performs automated evaluations using metrics like BLEU, ROUGE, context relevance, faithfulness, and more.</p>"},{"location":"cli/#help-and-subcommands","title":"Help and Subcommands","text":"<p>For help on a specific command, run:</p> <pre><code>poetry run python -m ragbot.cli &lt;command&gt; --help\n</code></pre> <p>For example:</p> <pre><code>poetry run python -m ragbot.cli chat --help\n</code></pre> <p>This will display detailed options and usage instructions for that subcommand.</p> <p>Refer to the API Reference for more information on available configuration options and evaluation metrics.</p>"},{"location":"reference/chat/","title":"Chat","text":"<p>Interactive chat interface for a RAG-powered language model.</p>"},{"location":"reference/chat/#ragbot.chat.chat","title":"<code>chat(project_name, llm_provider, llm, llm_temperature, llm_top_p, llm_top_k, embeddings_provider, embedding_model, chunk_size, chunk_overlap, search_type, k_docs)</code>","text":"<p>Start an interactive chat session using a RAG pipeline.</p> <p>This function initializes the RAG components with the given parameters and launches a loop that allows users to send queries to the model. It supports simple commands for help (<code>/?</code>), clearing history (<code>/clear</code>), and exiting (<code>/bye</code>).</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>The name of the LangChain project.</p> required <code>llm_provider</code> <code>str</code> <p>The LLM provider (e.g., \"google\", \"ollama\", \"hf\").</p> required <code>llm</code> <code>str</code> <p>The LLM model identifier.</p> required <code>llm_temperature</code> <code>float</code> <p>Sampling temperature for the LLM.</p> required <code>llm_top_p</code> <code>float</code> <p>Top-p nucleus sampling parameter.</p> required <code>llm_top_k</code> <code>int</code> <p>Top-k sampling parameter.</p> required <code>embeddings_provider</code> <code>str</code> <p>The provider for the embeddings model.</p> required <code>embedding_model</code> <code>str</code> <p>The embeddings model identifier.</p> required <code>chunk_size</code> <code>int</code> <p>Size of each document chunk for retrieval.</p> required <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping tokens between chunks.</p> required <code>search_type</code> <code>str</code> <p>The type of retrieval search to use.</p> required <code>k_docs</code> <code>int</code> <p>Number of top documents to retrieve for each query.</p> required Source code in <code>ragbot\\chat.py</code> <pre><code>def chat(\n    project_name: str,\n    llm_provider: str,\n    llm: str,\n    llm_temperature: float,\n    llm_top_p: float,\n    llm_top_k: int,\n    embeddings_provider: str,\n    embedding_model: str,\n    chunk_size: int,\n    chunk_overlap: int,\n    search_type: str,\n    k_docs: int,\n):\n    \"\"\"Start an interactive chat session using a RAG pipeline.\n\n    This function initializes the RAG components with the given parameters and launches\n    a loop that allows users to send queries to the model. It supports simple commands\n    for help (`/?`), clearing history (`/clear`), and exiting (`/bye`).\n\n    Args:\n        project_name: The name of the LangChain project.\n        llm_provider: The LLM provider (e.g., \"google\", \"ollama\", \"hf\").\n        llm: The LLM model identifier.\n        llm_temperature: Sampling temperature for the LLM.\n        llm_top_p: Top-p nucleus sampling parameter.\n        llm_top_k: Top-k sampling parameter.\n        embeddings_provider: The provider for the embeddings model.\n        embedding_model: The embeddings model identifier.\n        chunk_size: Size of each document chunk for retrieval.\n        chunk_overlap: Number of overlapping tokens between chunks.\n        search_type: The type of retrieval search to use.\n        k_docs: Number of top documents to retrieve for each query.\n    \"\"\"\n    qa = setup(\n        project_name=project_name,\n        llm_provider=llm_provider,\n        llm=llm,\n        llm_temperature=llm_temperature,\n        llm_top_p=llm_top_p,\n        llm_top_k=llm_top_k,\n        embeddings_provider=embeddings_provider,\n        embedding_model=embedding_model,\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        search_type=search_type,\n        k_docs=k_docs,\n    )\n\n    history = []\n    while True:\n        query = input(\"&gt;&gt;&gt; \")\n        if query == \"/?\":\n            print(\"Type /? to show this help\")\n            print(\"Type /bye to exit.\")\n            print(\"Type /clear to clear the chat history.\")\n            continue\n        if query == \"/clear\":\n            history = []\n            continue\n        if query.lower() == \"/bye\":\n            print(\"Session closed!\")\n            exit(0)\n        response = qa.invoke({\"history\": history, \"input\": query})\n        history.extend([(\"human\", query), (\"ai\", response[\"answer\"])])\n        display_markdown(response[\"answer\"])\n</code></pre>"},{"location":"reference/cli/","title":"CLI","text":"<p>Command-line interface (CLI) entry point for RAG-based chatbot and evaluation tool.</p>"},{"location":"reference/cli/#ragbot.cli.chat_command","title":"<code>chat_command(args)</code>","text":"<p>Execute the chat command with parsed CLI arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Parsed argparse namespace containing chat config.</p> required Source code in <code>ragbot\\cli.py</code> <pre><code>def chat_command(args: argparse.Namespace):\n    \"\"\"Execute the chat command with parsed CLI arguments.\n\n    Args:\n        args: Parsed argparse namespace containing chat config.\n    \"\"\"\n    chat(\n        project_name=args.proj,\n        llm_provider=args.llm_provider,\n        llm=args.llm,\n        llm_temperature=args.temperature,\n        llm_top_p=args.top_p,\n        llm_top_k=args.top_k,\n        embeddings_provider=args.emb_provider,\n        embedding_model=args.emb_model,\n        chunk_size=args.chunk_size,\n        chunk_overlap=args.chunk_overlap,\n        search_type=args.search_type,\n        k_docs=args.k,\n    )\n</code></pre>"},{"location":"reference/cli/#ragbot.cli.evaluate_command","title":"<code>evaluate_command(args)</code>","text":"<p>Execute the evaluate command with parsed CLI arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Parsed argparse namespace containing evaluation config.</p> required Source code in <code>ragbot\\cli.py</code> <pre><code>def evaluate_command(args: argparse.Namespace):\n    \"\"\"Execute the evaluate command with parsed CLI arguments.\n\n    Args:\n        args: Parsed argparse namespace containing evaluation config.\n    \"\"\"\n    evaluate(\n        project_name=args.proj,\n        config_path=args.config_path,\n        dataset_name=args.dataset_name,\n    )\n</code></pre>"},{"location":"reference/cli/#ragbot.cli.main","title":"<code>main()</code>","text":"<p>Main CLI entry point.</p> <p>Parses command-line arguments and invokes the corresponding function for either chat or evaluation mode.</p> Source code in <code>ragbot\\cli.py</code> <pre><code>def main():\n    \"\"\"Main CLI entry point.\n\n    Parses command-line arguments and invokes the corresponding function for either\n    chat or evaluation mode.\n    \"\"\"\n    parser = ArgumentParser(\n        prog=\"poetry run python -m ragbot.cli\",\n        description=\":::A RAG-powered chatbot CLI tool:::\",\n        usage=\"poetry run python -m ragbot.cli [-h] &lt;command&gt;\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        epilog=\"Use `ragbot.cli &lt;command&gt; --help` for more information on a command.\",\n    )\n    subparsers = parser.add_subparsers(title=\"commands\", dest=\"command\", required=True)\n\n    # Subcommand: chat\n    chat_parser = subparsers.add_parser(\n        \"chat\",\n        help=\"Run the chat interface\",\n        usage=\"poetry run python -m ragbot.cli chat -p &lt;project&gt; [options]\",\n        formatter_class=argparse.MetavarTypeHelpFormatter,\n    )\n    parse_chat_args(chat_parser)\n\n    # Subcommand: evaluate\n    eval_parser = subparsers.add_parser(\n        \"evaluate\",\n        help=\"Evaluate using a config file\",\n        usage=\"poetry run python -m ragbot.cli evaluate -p &lt;project&gt; [options]\",\n        formatter_class=argparse.MetavarTypeHelpFormatter,\n    )\n    parse_evaluate_args(eval_parser)\n\n    args = parser.parse_args()\n    use_langsmith(args.proj)\n\n    try:\n        args.func(args)\n    except Exception as e:\n        print(f\"[ERROR] {e}\")\n        exit(1)\n</code></pre>"},{"location":"reference/cli/#ragbot.cli.parse_chat_args","title":"<code>parse_chat_args(subparser)</code>","text":"<p>Add command-line arguments for the 'chat' subcommand.</p> <p>Parameters:</p> Name Type Description Default <code>subparser</code> <code>ArgumentParser</code> <p>A subparser object from argparse to attach the chat arguments to.</p> required Source code in <code>ragbot\\cli.py</code> <pre><code>def parse_chat_args(subparser: argparse.ArgumentParser):\n    \"\"\"Add command-line arguments for the 'chat' subcommand.\n\n    Args:\n        subparser: A subparser object from argparse to attach the chat arguments to.\n    \"\"\"\n    subparser.add_argument(\n        \"-p\", \"--proj\", type=str, required=True, help=\"Name of the project\"\n    )\n    subparser.add_argument(\n        \"--llm-provider\", type=str, default=\"google\", help=\"LLM provider\"\n    )\n    subparser.add_argument(\n        \"--llm\", type=str, default=\"gemini-1.5-flash\", help=\"LLM model\"\n    )\n    subparser.add_argument(\n        \"--temperature\", type=float, default=0.0, help=\"LLM temperature\"\n    )\n    subparser.add_argument(\"--top-p\", type=float, default=0.85, help=\"LLM top-p\")\n    subparser.add_argument(\"--top-k\", type=int, default=40, help=\"LLM top-k\")\n    subparser.add_argument(\n        \"--emb-provider\", type=str, default=\"google\", help=\"Embeddings provider\"\n    )\n    subparser.add_argument(\n        \"--emb-model\", type=str, default=\"models/embedding-001\", help=\"Embeddings model\"\n    )\n    subparser.add_argument(\n        \"--chunk-size\", type=int, default=3000, help=\"Text splitter chunk size\"\n    )\n    subparser.add_argument(\n        \"--chunk-overlap\", type=int, default=600, help=\"Text splitter chunk overlap\"\n    )\n    subparser.add_argument(\n        \"--search-type\", type=str, default=\"similarity\", help=\"Retrieval search type\"\n    )\n    subparser.add_argument(\n        \"--k\", type=int, default=4, help=\"Number of documents to retrieve\"\n    )\n    subparser.set_defaults(func=chat_command)\n</code></pre>"},{"location":"reference/cli/#ragbot.cli.parse_evaluate_args","title":"<code>parse_evaluate_args(subparser)</code>","text":"<p>Add command-line arguments for the 'evaluate' subcommand.</p> <p>Parameters:</p> Name Type Description Default <code>subparser</code> <code>ArgumentParser</code> <p>A subparser object from argparse to attach the evaluate arguments to.</p> required Source code in <code>ragbot\\cli.py</code> <pre><code>def parse_evaluate_args(subparser: argparse.ArgumentParser):\n    \"\"\"Add command-line arguments for the 'evaluate' subcommand.\n\n    Args:\n        subparser: A subparser object from argparse to attach the evaluate arguments to.\n    \"\"\"\n    subparser.add_argument(\n        \"-p\", \"--proj\", type=str, required=True, help=\"Name of the project\"\n    )\n    subparser.add_argument(\n        \"--config-path\",\n        type=str,\n        default=\"configs/default.json\",\n        help=\"Path to config file\",\n    )\n    subparser.add_argument(\n        \"--dataset-name\", type=str, default=\"ds-test\", help=\"Name of the dataset\"\n    )\n    subparser.set_defaults(func=evaluate_command)\n</code></pre>"},{"location":"reference/evaluate/","title":"Evaluate","text":"<p>Evaluation pipeline for RAG model performance using LangSmith metrics.</p>"},{"location":"reference/evaluate/#ragbot.evaluate.evaluate","title":"<code>evaluate(project_name, config_path, dataset_name)</code>","text":"<p>Run evaluation on a RAG setup using LangSmith metrics.</p> <p>Loads a configuration file to initialize a RAG chain and evaluates its performance on a dataset using a set of standard metrics.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>The name of the LangChain project used in LangSmith.</p> required <code>config_path</code> <code>str</code> <p>Path to the JSON configuration file defining the RAG setup.</p> required <code>dataset_name</code> <code>str</code> <p>The name of the dataset to be used for evaluation, registered in LangSmith.</p> required <p>Raises:</p> Type Description <code>IOError</code> <p>If the configuration file at <code>config_path</code> does not exist.</p> Source code in <code>ragbot\\evaluate.py</code> <pre><code>def evaluate(project_name: str, config_path: str, dataset_name: str):\n    \"\"\"Run evaluation on a RAG setup using LangSmith metrics.\n\n    Loads a configuration file to initialize a RAG chain and evaluates its\n    performance on a dataset using a set of standard metrics.\n\n    Args:\n        project_name: The name of the LangChain project used in LangSmith.\n        config_path: Path to the JSON configuration file defining the RAG setup.\n        dataset_name: The name of the dataset to be used for evaluation, registered in LangSmith.\n\n    Raises:\n        IOError: If the configuration file at `config_path` does not exist.\n    \"\"\"\n\n    # Define LLM and embedding model for evalution\n    llm = get_model(\"google\", \"gemini-2.0-flash\", temperature=0.0)\n    embeddings = get_embeddings(\"google\", \"models/embedding-001\")\n\n    # Wrap the metrics for evaluation with LangSmith\n    evaluators = [\n        EvaluatorChain(metric=metric, llm=llm, embeddings=embeddings)\n        for metric in [\n            BLEU(),\n            ROUGE(),\n            SemanticSimilarity(),\n            Faithfulness(),\n            AnswerRelevance(),\n            ContextRelevance(),\n        ]\n    ]\n\n    # Load configuration from json config\n    if os.path.exists(config_path):\n        with open(config_path, \"r\") as f:\n            config = json.load(f)\n            metadata = config\n    else:\n        raise IOError(f\"Configuration file {config_path} not found.\")\n\n    # Set up RAG, and run an evalution\n    rag_chain = setup(\n        project_name=project_name,\n        llm_provider=config[\"llm_provider\"],\n        llm=config[\"llm\"],\n        llm_temperature=config[\"llm_temperature\"],\n        llm_top_p=config[\"llm_top_p\"],\n        llm_top_k=config[\"llm_top_k\"],\n        embeddings_provider=config[\"embeddings_provider\"],\n        embedding_model=config[\"embedding_model\"],\n        chunk_size=config[\"chunk_size\"],\n        chunk_overlap=config[\"chunk_overlap\"],\n        search_type=config[\"search_type\"],\n        k_docs=config[\"k_docs\"],\n    )\n\n    # Add delay for quota management\n    def invoke_with_delay(*args, **kwargs):\n        time.sleep(12)\n        return rag_chain.invoke(*args, **kwargs)\n\n    # Run evaluation\n    langsmith.evaluate(\n        invoke_with_delay,\n        data=dataset_name,\n        evaluators=evaluators,\n        experiment_prefix=\"base\",\n        metadata=metadata,\n        max_concurrency=1,\n    )\n</code></pre>"},{"location":"reference/rag/","title":"RAG","text":"<p>RAG setup module for initializing retrieval-augmented generation chains.</p>"},{"location":"reference/rag/#ragbot.rag.create_docs","title":"<code>create_docs(project_name, chunk_size, chunk_overlap)</code>","text":"<p>Create and return documents for a given project.</p> <p>Loads, splits, and stores the content to be used for embedding and retrieval. The <code>project_name/</code> directory must exist under the <code>data</code> directory with its knowledge base of .txt files.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>The project identifier.</p> required <code>chunk_size</code> <code>int</code> <p>The maximum size of each text chunk.</p> required <code>chunk_overlap</code> <code>int</code> <p>The number of characters to overlap between chunks.</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>A list of <code>Document</code> objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>project_name</code> knowledge base is not found at data/.</p> <code>FileNotFoundError</code> <p>If the <code>project_name</code> knowledge base is empty.</p> <code>RuntimeError</code> <p>If it fails to read any file from the knowledge base.</p> Source code in <code>ragbot\\rag.py</code> <pre><code>def create_docs(\n    project_name: str, chunk_size: int, chunk_overlap: int\n) -&gt; list[Document]:\n    \"\"\"Create and return documents for a given project.\n\n    Loads, splits, and stores the content to be used for embedding and\n    retrieval. The `project_name/` directory must exist under the `data`\n    directory with its knowledge base of .txt files.\n\n    Args:\n        project_name: The project identifier.\n        chunk_size: The maximum size of each text chunk.\n        chunk_overlap: The number of characters to overlap between chunks.\n\n    Returns:\n        A list of `Document` objects.\n\n    Raises:\n        ValueError: If the `project_name` knowledge base is not found at data/.\n        FileNotFoundError: If the `project_name` knowledge base is empty.\n        RuntimeError: If it fails to read any file from the knowledge base.\n    \"\"\"\n    dir_path = f\"data/{project_name}\"\n    if not os.path.exists(dir_path):\n        raise ValueError(\n            f\"No data for project {project_name}, please add directory {dir_path} with knowledge base files\"\n        )\n\n    # Remove previous content\n    content_dir = f\"{dir_path}/content/\"\n    if os.path.exists(content_dir):\n        shutil.rmtree(content_dir)\n    os.makedirs(content_dir)\n\n    # Declare splitter\n    text_splitter = RecursiveCharacterTextSplitter(\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n    )\n\n    # Create documents\n    docs = []\n    knowledge_base = glob.glob(f\"{dir_path}/*.txt\")\n    if not knowledge_base:\n        raise FileNotFoundError(f\"No .txt files found in {dir_path}\")\n\n    for file in knowledge_base:\n        try:\n            with open(file, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n        except Exception as e:\n            raise RuntimeError(f\"Failed to read file {file}: {e}\") from e\n        splits = text_splitter.split_text(content)\n        docs.extend([Document(page_content=chunk) for chunk in splits])\n\n    # Store documents\n    for i, doc in enumerate(docs):\n        with open(f\"{dir_path}/content/doc{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n            f.write(doc.page_content)\n\n    return docs\n</code></pre>"},{"location":"reference/rag/#ragbot.rag.setup","title":"<code>setup(project_name, llm_provider, llm, llm_temperature, llm_top_p, llm_top_k, embeddings_provider, embedding_model, chunk_size, chunk_overlap, search_type, k_docs)</code>","text":"<p>Set up and return a RAG retrieval chain.</p> <p>Initializes a language model and embeddings, chunks input documents, and creates a retrieval-augmented generation chain using LangChain.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of the project..</p> required <code>llm_provider</code> <code>str</code> <p>Provider name for the language model (e.g., \"google\", \"ollama\").</p> required <code>llm</code> <code>str</code> <p>Identifier or model name for the language model.</p> required <code>llm_temperature</code> <code>float</code> <p>Sampling temperature for language generation.</p> required <code>llm_top_p</code> <code>float</code> <p>Nucleus sampling top-p value.</p> required <code>llm_top_k</code> <code>int</code> <p>Top-k sampling value.</p> required <code>embeddings_provider</code> <code>str</code> <p>Provider name for embeddings.</p> required <code>embedding_model</code> <code>str</code> <p>Identifier for the embeddings model.</p> required <code>chunk_size</code> <code>int</code> <p>Maximum number of characters per document chunk.</p> required <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping characters between chunks.</p> required <code>search_type</code> <code>str</code> <p>Type of search for the retriever (e.g., \"similarity\", \"mmr\").</p> required <code>k_docs</code> <code>int</code> <p>Number of top documents to retrieve.</p> required <p>Returns:</p> Type Description <code>Runnable</code> <p>A <code>Runnable</code> LangChain object that processes user input through a RAG pipeline.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If there is no <code>system.prompt</code> file at <code>data/project_name</code>.</p> <code>RuntimeError</code> <p>If the system prompt file cannot be read, or if it does not contain the required <code>{context}</code> placeholder.</p> Source code in <code>ragbot\\rag.py</code> <pre><code>def setup(\n    project_name: str,\n    llm_provider: str,\n    llm: str,\n    llm_temperature: float,\n    llm_top_p: float,\n    llm_top_k: int,\n    embeddings_provider: str,\n    embedding_model: str,\n    chunk_size: int,\n    chunk_overlap: int,\n    search_type: str,\n    k_docs: int,\n) -&gt; Runnable:\n    \"\"\"Set up and return a RAG retrieval chain.\n\n    Initializes a language model and embeddings, chunks input documents,\n    and creates a retrieval-augmented generation chain using LangChain.\n\n    Args:\n        project_name: Name of the project..\n        llm_provider: Provider name for the language model (e.g., \"google\", \"ollama\").\n        llm: Identifier or model name for the language model.\n        llm_temperature: Sampling temperature for language generation.\n        llm_top_p: Nucleus sampling top-p value.\n        llm_top_k: Top-k sampling value.\n        embeddings_provider: Provider name for embeddings.\n        embedding_model: Identifier for the embeddings model.\n        chunk_size: Maximum number of characters per document chunk.\n        chunk_overlap: Number of overlapping characters between chunks.\n        search_type: Type of search for the retriever (e.g., \"similarity\", \"mmr\").\n        k_docs: Number of top documents to retrieve.\n\n    Returns:\n        A `Runnable` LangChain object that processes user input through a RAG pipeline.\n\n    Raises:\n        FileNotFoundError: If there is no `system.prompt` file at `data/project_name`.\n        RuntimeError: If the system prompt file cannot be read, or if it does not contain the required `{context}` placeholder.\n    \"\"\"\n    # Set up a language model\n    llm = get_model(\n        llm_provider, llm, temperature=llm_temperature, top_p=llm_top_p, top_k=llm_top_k\n    )\n\n    # Create documents from knowledge base\n    docs = create_docs(project_name, chunk_size, chunk_overlap)\n\n    # Create vectorstore\n    vectorstore = Chroma.from_documents(\n        documents=docs,\n        embedding=get_embeddings(embeddings_provider, embedding_model),\n    )\n\n    # Instantiate the relevant docs retriever\n    retriever = vectorstore.as_retriever(\n        search_type=search_type, search_kwargs={\"k\": k_docs}\n    )\n\n    # Check system prompt and read\n    file = f\"data/{project_name}/system.prompt\"\n    if not os.path.exists(file):\n        raise FileNotFoundError(\n            f\"No system prompt for project {project_name}, please add file {file}\"\n        )\n\n    try:\n        with open(file, \"r\", encoding=\"utf-8\") as f:\n            system_prompt = f.read()\n    except Exception as e:\n        raise RuntimeError(f\"Failed to read file {file}: {e}\") from e\n\n    if not \"{context}\" in system_prompt:\n        raise RuntimeError(\n            f\"System prompt at {file} must have the {{context}} placeholder\"\n        )\n\n    # Create custom prompt\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", system_prompt),\n            MessagesPlaceholder(\n                variable_name=\"history\",\n                optional=True,\n                n_messages=10,\n            ),\n            (\"human\", \"{input}\"),\n        ]\n    )\n\n    # Create retrieval chain\n    qa_chain = create_stuff_documents_chain(llm, prompt)\n    rag_chain = create_retrieval_chain(retriever, qa_chain)\n\n    return rag_chain\n</code></pre>"},{"location":"reference/evaluation/dataset_schema/","title":"Dataset Schema","text":"<p>Schema definitions for evaluation samples used in RAG evaluation.</p>"},{"location":"reference/evaluation/dataset_schema/#ragbot.evaluation.dataset_schema.BaseSample","title":"<code>BaseSample</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for evaluation samples providing feature extraction.</p> Source code in <code>ragbot\\evaluation\\dataset_schema.py</code> <pre><code>class BaseSample(BaseModel):\n    \"\"\"Base class for evaluation samples providing feature extraction.\"\"\"\n\n    def get_features(self):\n        return list(self.to_dict().keys())\n</code></pre>"},{"location":"reference/evaluation/dataset_schema/#ragbot.evaluation.dataset_schema.Sample","title":"<code>Sample</code>","text":"<p>               Bases: <code>BaseSample</code></p> <p>Structured sample used for evaluating RAG responses.</p> <p>Attributes:</p> Name Type Description <code>question</code> <code>Optional[str]</code> <p>The user's question or input.</p> <code>answer</code> <code>Optional[str]</code> <p>The model-generated answer.</p> <code>retrieved_context</code> <code>Optional[List[str]]</code> <p>The context retrieved by the RAG pipeline.</p> <code>reference_context</code> <code>Optional[List[str]]</code> <p>The ground truth context.</p> <code>reference_answer</code> <code>Optional[str]</code> <p>The ground truth answer.</p> Source code in <code>ragbot\\evaluation\\dataset_schema.py</code> <pre><code>class Sample(BaseSample):\n    \"\"\"Structured sample used for evaluating RAG responses.\n\n    Attributes:\n        question (Optional[str]): The user's question or input.\n        answer (Optional[str]): The model-generated answer.\n        retrieved_context (Optional[List[str]]): The context retrieved by the RAG pipeline.\n        reference_context (Optional[List[str]]): The ground truth context.\n        reference_answer (Optional[str]): The ground truth answer.\n    \"\"\"\n\n    question: Optional[str] = None\n    answer: Optional[str] = None\n    retrieved_context: Optional[List[str]] = None\n    reference_context: Optional[List[str]] = None\n    reference_answer: Optional[str] = None\n</code></pre>"},{"location":"reference/evaluation/eval_chain/","title":"Evaluation Chain","text":"<p>EvaluatorChain for assessing the performance of a RAG pipeline.</p>"},{"location":"reference/evaluation/eval_chain/#ragbot.evaluation.eval_chain.EvaluatorChain","title":"<code>EvaluatorChain</code>","text":"<p>               Bases: <code>Chain</code>, <code>RunEvaluator</code></p> <p>Chain used to evaluate RAG outputs using custom metrics.</p> <p>This chain wraps around a scoring metric (e.g., semantic similarity, BLEU) and applies it to a structured sample. It is compatible with LangSmith's evaluation framework.</p> <p>Attributes:</p> Name Type Description <code>metric</code> <code>Metric</code> <p>The scoring metric used for evaluation. Can support LLM or embedding models.</p> Source code in <code>ragbot\\evaluation\\eval_chain.py</code> <pre><code>class EvaluatorChain(Chain, RunEvaluator):\n    \"\"\"Chain used to evaluate RAG outputs using custom metrics.\n\n    This chain wraps around a scoring metric (e.g., semantic similarity, BLEU)\n    and applies it to a structured sample. It is compatible with LangSmith's\n    evaluation framework.\n\n    Attributes:\n        metric (Metric): The scoring metric used for evaluation. Can support LLM or embedding models.\n    \"\"\"\n\n    metric: Metric\n\n    def __init__(self, metric: Metric, **kwargs: Any):\n        \"\"\"Initializes the EvaluatorChain.\n\n        Args:\n            metric (Metric): The evaluation metric instance.\n            **kwargs: Optional keyword arguments, including 'llm' or 'embeddings' if required by the metric.\n        \"\"\"\n        kwargs[\"metric\"] = metric\n        super().__init__(**kwargs)\n        if isinstance(self.metric, MetricWithLLM):\n            cast(MetricWithLLM, self.metric).llm = kwargs.get(\"llm\")\n        if isinstance(self.metric, MetricWithEmbeddings):\n            cast(MetricWithEmbeddings, self.metric).embeddings = kwargs.get(\n                \"embeddings\"\n            )\n        self.metric.init()\n\n    @property\n    def input_keys(self) -&gt; List[str]:\n        \"\"\"Defines input keys required by the metric.\n\n        Returns:\n            List[str]: Required column names for the metric.\n        \"\"\"\n        return list(self.metric.required_columns)\n\n    @property\n    def output_keys(self) -&gt; List[str]:\n        \"\"\"Defines the output keys produced by the evaluation.\n\n        Returns:\n            List[str]: The name of the metric used as the output key.\n        \"\"\"\n        return [self.metric.name]\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Runs the evaluation chain with provided inputs.\n\n        Args:\n            inputs (Dict[str, Any]): Input dictionary for the metric.\n            run_manager (Optional[CallbackManagerForChainRun]): Callback manager for tracking.\n\n        Returns:\n            Dict[str, Any]: The computed evaluation score.\n        \"\"\"\n        sample = Sample(**inputs)\n        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n        callbacks = _run_manager.get_child()\n        score = self.metric.score(sample, callbacks=callbacks)\n        print(f\"{self.metric.name}: {score}\")\n        return {self.metric.name: score}\n\n    def evaluate_run(\n        self, run: Run, example: Optional[Example] = None\n    ) -&gt; Union[EvaluationResult, EvaluationResults]:\n        \"\"\"Evaluates a single run against a reference example.\n\n        Args:\n            run (Run): The RAG pipeline run to evaluate.\n            example (Optional[Example]): Ground truth data for evaluation.\n\n        Returns:\n            Union[EvaluationResult, EvaluationResults]: Evaluation results for the run.\n        \"\"\"\n        chain_eval = self._chain_eval(example, run)\n        eval_output = self.invoke(chain_eval, include_run_info=True)\n        eval_result = EvaluationResult(\n            key=self.metric.name,\n            score=eval_output[self.metric.name],\n        )\n\n        # Make the view of the run available in LangSmith\n        if RUN_KEY in eval_output:\n            eval_result.evaluator_info[RUN_KEY] = eval_output[RUN_KEY]\n\n        return eval_result\n\n    def _chain_eval(self, example: Example, run: Run) -&gt; Dict:\n        \"\"\"Builds input dictionary for evaluation from run and example.\n\n        Args:\n            example (Example): Ground truth sample.\n            run (Run): Output run to be evaluated.\n\n        Returns:\n            Dict: Input values expected by the metric.\n        \"\"\"\n        chain_eval = dict()\n\n        if \"question\" in self.metric.required_columns:\n            chain_eval[\"question\"] = run.outputs[\"input\"]\n        if \"retrieved_context\" in self.metric.required_columns:\n            chain_eval[\"retrieved_context\"] = [\n                context.page_content for context in run.outputs[\"context\"]\n            ]\n        if \"reference_context\" in self.metric.required_columns:\n            chain_eval[\"reference_context\"] = example.outputs[\"context\"]\n        if \"answer\" in self.metric.required_columns:\n            chain_eval[\"answer\"] = run.outputs[\"answer\"]\n        if \"reference_answer\" in self.metric.required_columns:\n            chain_eval[\"reference_answer\"] = example.outputs[\"output\"]\n\n        return chain_eval\n</code></pre>"},{"location":"reference/evaluation/eval_chain/#ragbot.evaluation.eval_chain.EvaluatorChain.input_keys","title":"<code>input_keys</code>  <code>property</code>","text":"<p>Defines input keys required by the metric.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Required column names for the metric.</p>"},{"location":"reference/evaluation/eval_chain/#ragbot.evaluation.eval_chain.EvaluatorChain.output_keys","title":"<code>output_keys</code>  <code>property</code>","text":"<p>Defines the output keys produced by the evaluation.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The name of the metric used as the output key.</p>"},{"location":"reference/evaluation/eval_chain/#ragbot.evaluation.eval_chain.EvaluatorChain.__init__","title":"<code>__init__(metric, **kwargs)</code>","text":"<p>Initializes the EvaluatorChain.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Metric</code> <p>The evaluation metric instance.</p> required <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments, including 'llm' or 'embeddings' if required by the metric.</p> <code>{}</code> Source code in <code>ragbot\\evaluation\\eval_chain.py</code> <pre><code>def __init__(self, metric: Metric, **kwargs: Any):\n    \"\"\"Initializes the EvaluatorChain.\n\n    Args:\n        metric (Metric): The evaluation metric instance.\n        **kwargs: Optional keyword arguments, including 'llm' or 'embeddings' if required by the metric.\n    \"\"\"\n    kwargs[\"metric\"] = metric\n    super().__init__(**kwargs)\n    if isinstance(self.metric, MetricWithLLM):\n        cast(MetricWithLLM, self.metric).llm = kwargs.get(\"llm\")\n    if isinstance(self.metric, MetricWithEmbeddings):\n        cast(MetricWithEmbeddings, self.metric).embeddings = kwargs.get(\n            \"embeddings\"\n        )\n    self.metric.init()\n</code></pre>"},{"location":"reference/evaluation/eval_chain/#ragbot.evaluation.eval_chain.EvaluatorChain.evaluate_run","title":"<code>evaluate_run(run, example=None)</code>","text":"<p>Evaluates a single run against a reference example.</p> <p>Parameters:</p> Name Type Description Default <code>run</code> <code>Run</code> <p>The RAG pipeline run to evaluate.</p> required <code>example</code> <code>Optional[Example]</code> <p>Ground truth data for evaluation.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[EvaluationResult, EvaluationResults]</code> <p>Union[EvaluationResult, EvaluationResults]: Evaluation results for the run.</p> Source code in <code>ragbot\\evaluation\\eval_chain.py</code> <pre><code>def evaluate_run(\n    self, run: Run, example: Optional[Example] = None\n) -&gt; Union[EvaluationResult, EvaluationResults]:\n    \"\"\"Evaluates a single run against a reference example.\n\n    Args:\n        run (Run): The RAG pipeline run to evaluate.\n        example (Optional[Example]): Ground truth data for evaluation.\n\n    Returns:\n        Union[EvaluationResult, EvaluationResults]: Evaluation results for the run.\n    \"\"\"\n    chain_eval = self._chain_eval(example, run)\n    eval_output = self.invoke(chain_eval, include_run_info=True)\n    eval_result = EvaluationResult(\n        key=self.metric.name,\n        score=eval_output[self.metric.name],\n    )\n\n    # Make the view of the run available in LangSmith\n    if RUN_KEY in eval_output:\n        eval_result.evaluator_info[RUN_KEY] = eval_output[RUN_KEY]\n\n    return eval_result\n</code></pre>"},{"location":"reference/evaluation/metrics/answer_relevance/","title":"Answer Relevance","text":"<p>Evaluation metric for answer relevance in RAG systems.</p>"},{"location":"reference/evaluation/metrics/answer_relevance/#ragbot.evaluation.metrics.answer_relevance.AnswerRelevance","title":"<code>AnswerRelevance</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code></p> <p>Metric to evaluate the relevance of a generated answer to a user question.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the metric.</p> Source code in <code>ragbot\\evaluation\\metrics\\answer_relevance.py</code> <pre><code>@dataclass\nclass AnswerRelevance(MetricWithLLM):\n    \"\"\"Metric to evaluate the relevance of a generated answer to a user question.\n\n    Attributes:\n        name (str): The name of the metric.\n    \"\"\"\n\n    name: str = field(default=\"answer relevance\", repr=True)\n    _required_columns: Set[str] = field(default_factory=lambda: {\"question\", \"answer\"})\n\n    def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n        \"\"\"Score the answer relevance using a language model.\n\n        The score is based on a scale from 1 to 5:\n            5 - Excellent\n            4 - Good\n            3 - Acceptable\n            2 - Poor\n            1 - Not Relevant\n\n        Args:\n            sample (Sample): A sample containing a question and generated answer.\n            **kwargs: Additional keyword arguments (e.g., for callbacks).\n\n        Returns:\n            float: A relevance score between 1 and 5.\n        \"\"\"\n        question, answer = sample.question, sample.answer\n        output = self.llm.invoke(\n            f\"\"\"\n            You are an expert evaluator assessing how relevant a given answer is to a user question. \n            Answer relevance is defined as how well the response aligns with the user input. \n\n            **Scoring Guidelines:**  \n            - **5 (Excellent):** The answer fully addresses the question with clear, relevant, and complete information.  \n            - **4 (Good):** The answer is mostly relevant but may miss minor details or include slight redundancy.  \n            - **3 (Acceptable):** The answer is somewhat relevant but may be incomplete or contain noticeable redundancy.  \n            - **2 (Poor):** The answer only partially addresses the question and includes significant irrelevant or missing content.  \n            - **1 (Not Relevant):** The answer does not address the question or is entirely off-topic.  \n\n            **User Question:** {question}  \n            **Generated Answer:** {answer}  \n\n            Assign a single integer score (1-5) based on the above criteria.\n            Only return the score as a number, without any extra text.\n\n            Verdict [1 | 2 | 3 | 4 | 5]: \n            \"\"\"\n        )\n\n        score = int(output.content.strip())\n        return score\n</code></pre>"},{"location":"reference/evaluation/metrics/answer_relevance/#ragbot.evaluation.metrics.answer_relevance.AnswerRelevance.score","title":"<code>score(sample, **kwargs)</code>","text":"<p>Score the answer relevance using a language model.</p> The score is based on a scale from 1 to 5 <p>5 - Excellent 4 - Good 3 - Acceptable 2 - Poor 1 - Not Relevant</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Sample</code> <p>A sample containing a question and generated answer.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (e.g., for callbacks).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A relevance score between 1 and 5.</p> Source code in <code>ragbot\\evaluation\\metrics\\answer_relevance.py</code> <pre><code>def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n    \"\"\"Score the answer relevance using a language model.\n\n    The score is based on a scale from 1 to 5:\n        5 - Excellent\n        4 - Good\n        3 - Acceptable\n        2 - Poor\n        1 - Not Relevant\n\n    Args:\n        sample (Sample): A sample containing a question and generated answer.\n        **kwargs: Additional keyword arguments (e.g., for callbacks).\n\n    Returns:\n        float: A relevance score between 1 and 5.\n    \"\"\"\n    question, answer = sample.question, sample.answer\n    output = self.llm.invoke(\n        f\"\"\"\n        You are an expert evaluator assessing how relevant a given answer is to a user question. \n        Answer relevance is defined as how well the response aligns with the user input. \n\n        **Scoring Guidelines:**  \n        - **5 (Excellent):** The answer fully addresses the question with clear, relevant, and complete information.  \n        - **4 (Good):** The answer is mostly relevant but may miss minor details or include slight redundancy.  \n        - **3 (Acceptable):** The answer is somewhat relevant but may be incomplete or contain noticeable redundancy.  \n        - **2 (Poor):** The answer only partially addresses the question and includes significant irrelevant or missing content.  \n        - **1 (Not Relevant):** The answer does not address the question or is entirely off-topic.  \n\n        **User Question:** {question}  \n        **Generated Answer:** {answer}  \n\n        Assign a single integer score (1-5) based on the above criteria.\n        Only return the score as a number, without any extra text.\n\n        Verdict [1 | 2 | 3 | 4 | 5]: \n        \"\"\"\n    )\n\n    score = int(output.content.strip())\n    return score\n</code></pre>"},{"location":"reference/evaluation/metrics/base/","title":"Base","text":"<p>Base classes for defining RAG evaluation metrics.</p>"},{"location":"reference/evaluation/metrics/base/#ragbot.evaluation.metrics.base.Metric","title":"<code>Metric</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for evaluation metrics.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the metric.</p> Source code in <code>ragbot\\evaluation\\metrics\\base.py</code> <pre><code>@dataclass\nclass Metric(ABC):\n    \"\"\"Abstract base class for evaluation metrics.\n\n    Attributes:\n        name (str): Name of the metric.\n    \"\"\"\n\n    name: str = field(default=\"\", repr=True)\n    _required_columns: Set[str] = field(default_factory=set)\n\n    def init(self):\n        \"\"\"Optional initializer hook for the metric.\"\"\"\n        pass\n\n    @property\n    def required_columns(self) -&gt; Set[str]:\n        \"\"\"Set of columns required in the input sample to compute the metric.\n\n        Returns:\n            Set[str]: Required column names.\n        \"\"\"\n        return self._required_columns\n\n    @required_columns.setter\n    def required_columns(self, required_columns: Set[str]):\n        \"\"\"Set the required columns.\n\n        Args:\n            required_columns (Set[str]): Column names to be required.\n        \"\"\"\n        self._required_columns = required_columns\n\n    @abstractmethod\n    def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n        \"\"\"Compute a score for a given sample.\n\n        Args:\n            sample (Sample): Input data to evaluate.\n            **kwargs: Additional arguments (e.g., for callbacks or config).\n\n        Returns:\n            float: Computed metric score.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/evaluation/metrics/base/#ragbot.evaluation.metrics.base.Metric.required_columns","title":"<code>required_columns</code>  <code>property</code> <code>writable</code>","text":"<p>Set of columns required in the input sample to compute the metric.</p> <p>Returns:</p> Type Description <code>Set[str]</code> <p>Set[str]: Required column names.</p>"},{"location":"reference/evaluation/metrics/base/#ragbot.evaluation.metrics.base.Metric.init","title":"<code>init()</code>","text":"<p>Optional initializer hook for the metric.</p> Source code in <code>ragbot\\evaluation\\metrics\\base.py</code> <pre><code>def init(self):\n    \"\"\"Optional initializer hook for the metric.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/evaluation/metrics/base/#ragbot.evaluation.metrics.base.Metric.score","title":"<code>score(sample, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Compute a score for a given sample.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Sample</code> <p>Input data to evaluate.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments (e.g., for callbacks or config).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Computed metric score.</p> Source code in <code>ragbot\\evaluation\\metrics\\base.py</code> <pre><code>@abstractmethod\ndef score(self, sample: Sample, **kwargs: Any) -&gt; float:\n    \"\"\"Compute a score for a given sample.\n\n    Args:\n        sample (Sample): Input data to evaluate.\n        **kwargs: Additional arguments (e.g., for callbacks or config).\n\n    Returns:\n        float: Computed metric score.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/evaluation/metrics/base/#ragbot.evaluation.metrics.base.MetricWithEmbeddings","title":"<code>MetricWithEmbeddings</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Metric</code>, <code>ABC</code></p> <p>Base class for metrics that require embeddings to operate.</p> Source code in <code>ragbot\\evaluation\\metrics\\base.py</code> <pre><code>class MetricWithEmbeddings(Metric, ABC):\n    \"\"\"Base class for metrics that require embeddings to operate.\"\"\"\n\n    embeddings: Embeddings = None\n\n    def init(self):\n        \"\"\"Check that embeddings are set before scoring.\"\"\"\n        if self.embeddings is None:\n            raise ValueError(f\"Embeddings not set for Metric {self.name}\")\n</code></pre>"},{"location":"reference/evaluation/metrics/base/#ragbot.evaluation.metrics.base.MetricWithEmbeddings.init","title":"<code>init()</code>","text":"<p>Check that embeddings are set before scoring.</p> Source code in <code>ragbot\\evaluation\\metrics\\base.py</code> <pre><code>def init(self):\n    \"\"\"Check that embeddings are set before scoring.\"\"\"\n    if self.embeddings is None:\n        raise ValueError(f\"Embeddings not set for Metric {self.name}\")\n</code></pre>"},{"location":"reference/evaluation/metrics/base/#ragbot.evaluation.metrics.base.MetricWithLLM","title":"<code>MetricWithLLM</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Metric</code>, <code>ABC</code></p> <p>Base class for metrics that require an LLM to operate.</p> Source code in <code>ragbot\\evaluation\\metrics\\base.py</code> <pre><code>class MetricWithLLM(Metric, ABC):\n    \"\"\"Base class for metrics that require an LLM to operate.\"\"\"\n\n    llm: BaseChatModel | LLM = None\n\n    def init(self):\n        \"\"\"Check that the LLM is set before scoring.\"\"\"\n        if self.llm is None:\n            raise ValueError(f\"LLM not set for Metric {self.name}\")\n</code></pre>"},{"location":"reference/evaluation/metrics/base/#ragbot.evaluation.metrics.base.MetricWithLLM.init","title":"<code>init()</code>","text":"<p>Check that the LLM is set before scoring.</p> Source code in <code>ragbot\\evaluation\\metrics\\base.py</code> <pre><code>def init(self):\n    \"\"\"Check that the LLM is set before scoring.\"\"\"\n    if self.llm is None:\n        raise ValueError(f\"LLM not set for Metric {self.name}\")\n</code></pre>"},{"location":"reference/evaluation/metrics/bleu/","title":"BLEU","text":"<p>BLEU score metric for evaluating text similarity in RAG systems.</p>"},{"location":"reference/evaluation/metrics/bleu/#ragbot.evaluation.metrics.bleu.BLEU","title":"<code>BLEU</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Metric</code></p> <p>BLEU score metric for comparing generated and reference answers.</p> <p>This implementation uses the <code>sacrebleu.corpus_bleu</code> method to compute a BLEU score for each answer-reference pair.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the metric.</p> Source code in <code>ragbot\\evaluation\\metrics\\bleu.py</code> <pre><code>@dataclass\nclass BLEU(Metric):\n    \"\"\"BLEU score metric for comparing generated and reference answers.\n\n    This implementation uses the `sacrebleu.corpus_bleu` method to compute\n    a BLEU score for each answer-reference pair.\n\n    Attributes:\n        name (str): The name of the metric.\n    \"\"\"\n\n    name: str = field(default=\"bleu\", repr=True)\n    _required_columns: Set[str] = field(\n        default_factory=lambda: {\"answer\", \"reference_answer\"}\n    )\n\n    def __post_init__(self):\n        \"\"\"Initialize BLEU metric and ensure `sacrebleu` is available.\"\"\"\n        try:\n            from sacrebleu import corpus_bleu\n        except ImportError as e:\n            raise ImportError(\n                f\"{e.name} is required. Please install it with `pip install {e.name}`\"\n            )\n        self.corpus_bleu = corpus_bleu\n\n    def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n        \"\"\"Compute BLEU score for a given sample.\n\n        Args:\n            sample (Sample): A sample containing `answer` and `reference_answer`.\n            **kwargs: Optional keyword arguments (unused here).\n\n        Returns:\n            float: BLEU score as a float between 0 and 1.\n        \"\"\"\n        reference, answer = sample.reference_answer, sample.answer\n        ref_sentences = reference.split(\". \")\n        ans_sentences = answer.split(\". \")\n\n        reference = [[reference] for reference in ref_sentences]\n        answer = ans_sentences\n        score = self.corpus_bleu(answer, reference).score / 100\n        return score\n</code></pre>"},{"location":"reference/evaluation/metrics/bleu/#ragbot.evaluation.metrics.bleu.BLEU.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialize BLEU metric and ensure <code>sacrebleu</code> is available.</p> Source code in <code>ragbot\\evaluation\\metrics\\bleu.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize BLEU metric and ensure `sacrebleu` is available.\"\"\"\n    try:\n        from sacrebleu import corpus_bleu\n    except ImportError as e:\n        raise ImportError(\n            f\"{e.name} is required. Please install it with `pip install {e.name}`\"\n        )\n    self.corpus_bleu = corpus_bleu\n</code></pre>"},{"location":"reference/evaluation/metrics/bleu/#ragbot.evaluation.metrics.bleu.BLEU.score","title":"<code>score(sample, **kwargs)</code>","text":"<p>Compute BLEU score for a given sample.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Sample</code> <p>A sample containing <code>answer</code> and <code>reference_answer</code>.</p> required <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments (unused here).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>BLEU score as a float between 0 and 1.</p> Source code in <code>ragbot\\evaluation\\metrics\\bleu.py</code> <pre><code>def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n    \"\"\"Compute BLEU score for a given sample.\n\n    Args:\n        sample (Sample): A sample containing `answer` and `reference_answer`.\n        **kwargs: Optional keyword arguments (unused here).\n\n    Returns:\n        float: BLEU score as a float between 0 and 1.\n    \"\"\"\n    reference, answer = sample.reference_answer, sample.answer\n    ref_sentences = reference.split(\". \")\n    ans_sentences = answer.split(\". \")\n\n    reference = [[reference] for reference in ref_sentences]\n    answer = ans_sentences\n    score = self.corpus_bleu(answer, reference).score / 100\n    return score\n</code></pre>"},{"location":"reference/evaluation/metrics/context_relevance/","title":"Context Relevance","text":"<p>Context relevance metric for evaluating the relevance of retrieved context.</p>"},{"location":"reference/evaluation/metrics/context_relevance/#ragbot.evaluation.metrics.context_relevance.ContextRelevance","title":"<code>ContextRelevance</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code></p> <p>Context relevance metric for RAG evaluation.</p> <p>This metric evaluates the relevance of the retrieved context to the user's question based on a scoring system from 1 to 5. The metric uses an LLM to assess how well the context aligns with the question.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the metric.</p> Source code in <code>ragbot\\evaluation\\metrics\\context_relevance.py</code> <pre><code>@dataclass\nclass ContextRelevance(MetricWithLLM):\n    \"\"\"Context relevance metric for RAG evaluation.\n\n    This metric evaluates the relevance of the retrieved context to the user's question\n    based on a scoring system from 1 to 5. The metric uses an LLM to assess how well\n    the context aligns with the question.\n\n    Attributes:\n        name (str): The name of the metric.\n    \"\"\"\n\n    name: str = field(default=\"context relevance\", repr=True)\n    _required_columns: Set[str] = field(\n        default_factory=lambda: {\"question\", \"retrieved_context\"}\n    )\n\n    def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n        \"\"\"Compute the context relevance score for a given sample.\n\n        Args:\n            sample (Sample): A sample containing the user question and retrieved context.\n            **kwargs: Optional keyword arguments (not used here).\n\n        Returns:\n            float: A score (1-5) indicating how relevant the retrieved context is to the user's question.\n        \"\"\"\n        question, context = sample.question, sample.retrieved_context\n        output = self.llm.invoke(\n            f\"\"\"\n            You are an expert evaluator assessing how relevant a retrieved context is to a given user question. \n            Context relevance is defined as how well the retrieved information aligns with the question. \n\n            **Scoring Guidelines:**  \n            - **5 (Excellent):** The context is fully relevant to the question, containing directly useful and necessary information.  \n            - **4 (Good):** The context is mostly relevant but may include minor irrelevant details or miss slight nuances.  \n            - **3 (Acceptable):** The context is somewhat relevant but may include noticeable irrelevant parts or lack some important details.  \n            - **2 (Poor):** The context is only partially relevant, with significant irrelevant or missing information.  \n            - **1 (Not Relevant):** The context is mostly or entirely unrelated to the question.  \n\n            **User Question:** {question}  \n            **Retrieved Context:** {context}  \n\n            Assign a single integer score (1-5) based on the above criteria.\n            Only return the score as a number, without any extra text.\n\n            Verdict [1 | 2 | 3 | 4 | 5]: \n            \"\"\"\n        )\n\n        score = int(output.content.strip())\n        return score\n</code></pre>"},{"location":"reference/evaluation/metrics/context_relevance/#ragbot.evaluation.metrics.context_relevance.ContextRelevance.score","title":"<code>score(sample, **kwargs)</code>","text":"<p>Compute the context relevance score for a given sample.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Sample</code> <p>A sample containing the user question and retrieved context.</p> required <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments (not used here).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A score (1-5) indicating how relevant the retrieved context is to the user's question.</p> Source code in <code>ragbot\\evaluation\\metrics\\context_relevance.py</code> <pre><code>def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n    \"\"\"Compute the context relevance score for a given sample.\n\n    Args:\n        sample (Sample): A sample containing the user question and retrieved context.\n        **kwargs: Optional keyword arguments (not used here).\n\n    Returns:\n        float: A score (1-5) indicating how relevant the retrieved context is to the user's question.\n    \"\"\"\n    question, context = sample.question, sample.retrieved_context\n    output = self.llm.invoke(\n        f\"\"\"\n        You are an expert evaluator assessing how relevant a retrieved context is to a given user question. \n        Context relevance is defined as how well the retrieved information aligns with the question. \n\n        **Scoring Guidelines:**  \n        - **5 (Excellent):** The context is fully relevant to the question, containing directly useful and necessary information.  \n        - **4 (Good):** The context is mostly relevant but may include minor irrelevant details or miss slight nuances.  \n        - **3 (Acceptable):** The context is somewhat relevant but may include noticeable irrelevant parts or lack some important details.  \n        - **2 (Poor):** The context is only partially relevant, with significant irrelevant or missing information.  \n        - **1 (Not Relevant):** The context is mostly or entirely unrelated to the question.  \n\n        **User Question:** {question}  \n        **Retrieved Context:** {context}  \n\n        Assign a single integer score (1-5) based on the above criteria.\n        Only return the score as a number, without any extra text.\n\n        Verdict [1 | 2 | 3 | 4 | 5]: \n        \"\"\"\n    )\n\n    score = int(output.content.strip())\n    return score\n</code></pre>"},{"location":"reference/evaluation/metrics/faithfulness/","title":"Faithfulness","text":"<p>Faithfulness metric for evaluating the accuracy of generated answers based on the retrieved context.</p>"},{"location":"reference/evaluation/metrics/faithfulness/#ragbot.evaluation.metrics.faithfulness.Faithfulness","title":"<code>Faithfulness</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithLLM</code></p> <p>Faithfulness metric for RAG evaluation.</p> <p>This metric evaluates how well the generated answer is grounded in the provided retrieved context. It uses an LLM to score the answer based on the degree to which it reflects the information in the context.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the metric.</p> Source code in <code>ragbot\\evaluation\\metrics\\faithfulness.py</code> <pre><code>@dataclass\nclass Faithfulness(MetricWithLLM):\n    \"\"\"Faithfulness metric for RAG evaluation.\n\n    This metric evaluates how well the generated answer is grounded in the provided retrieved context.\n    It uses an LLM to score the answer based on the degree to which it reflects the information in the context.\n\n    Attributes:\n        name (str): The name of the metric.\n    \"\"\"\n\n    name: str = field(default=\"faithfulness\", repr=True)\n    _required_columns: Set[str] = field(\n        default_factory=lambda: {\"answer\", \"retrieved_context\"}\n    )\n\n    def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n        \"\"\"Compute the faithfulness score for a given sample.\n\n        This method evaluates how grounded the generated answer is in the provided context.\n\n        Args:\n            sample (Sample): A sample containing the retrieved context and generated answer.\n            **kwargs: Optional keyword arguments (not used here).\n\n        Returns:\n            float: A score (1-5) indicating how well the answer is grounded in the retrieved context.\n        \"\"\"\n\n        context, answer = sample.retrieved_context, sample.answer\n        output = self.llm.invoke(\n            f\"\"\"\n            You are an expert evaluator assessing how well a generated answer is grounded in the provided retrieved context. \n            Groundedness is defined as how accurately the answer reflects the information in the retrieved context, without adding unsupported details or hallucinating information.\n\n            **Scoring Guidelines:**  \n            - **5 (Excellent):** The answer is fully grounded in the context, with no hallucinations or unsupported claims.  \n            - **4 (Good):** The answer is mostly grounded but may have minor phrasing variations or slight extrapolations.  \n            - **3 (Acceptable):** The answer is somewhat grounded but includes noticeable extrapolations or minor unsupported details.  \n            - **2 (Poor):** The answer contains significant ungrounded information or misinterprets key details from the context.  \n            - **1 (Not Grounded):** The answer is mostly or entirely hallucinated, containing little to no support from the provided context.  \n\n            **Retrieved Context:** {context}  \n            **Generated Answer:** {answer}  \n\n            Assign a single integer score (1-5) based on the above criteria.\n            Only return the score as a number, without any extra text.\n\n            Verdict [1 | 2 | 3 | 4 | 5]: \n            \"\"\"\n        )\n\n        score = int(output.content.strip())\n        return score\n</code></pre>"},{"location":"reference/evaluation/metrics/faithfulness/#ragbot.evaluation.metrics.faithfulness.Faithfulness.score","title":"<code>score(sample, **kwargs)</code>","text":"<p>Compute the faithfulness score for a given sample.</p> <p>This method evaluates how grounded the generated answer is in the provided context.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Sample</code> <p>A sample containing the retrieved context and generated answer.</p> required <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments (not used here).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A score (1-5) indicating how well the answer is grounded in the retrieved context.</p> Source code in <code>ragbot\\evaluation\\metrics\\faithfulness.py</code> <pre><code>def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n    \"\"\"Compute the faithfulness score for a given sample.\n\n    This method evaluates how grounded the generated answer is in the provided context.\n\n    Args:\n        sample (Sample): A sample containing the retrieved context and generated answer.\n        **kwargs: Optional keyword arguments (not used here).\n\n    Returns:\n        float: A score (1-5) indicating how well the answer is grounded in the retrieved context.\n    \"\"\"\n\n    context, answer = sample.retrieved_context, sample.answer\n    output = self.llm.invoke(\n        f\"\"\"\n        You are an expert evaluator assessing how well a generated answer is grounded in the provided retrieved context. \n        Groundedness is defined as how accurately the answer reflects the information in the retrieved context, without adding unsupported details or hallucinating information.\n\n        **Scoring Guidelines:**  \n        - **5 (Excellent):** The answer is fully grounded in the context, with no hallucinations or unsupported claims.  \n        - **4 (Good):** The answer is mostly grounded but may have minor phrasing variations or slight extrapolations.  \n        - **3 (Acceptable):** The answer is somewhat grounded but includes noticeable extrapolations or minor unsupported details.  \n        - **2 (Poor):** The answer contains significant ungrounded information or misinterprets key details from the context.  \n        - **1 (Not Grounded):** The answer is mostly or entirely hallucinated, containing little to no support from the provided context.  \n\n        **Retrieved Context:** {context}  \n        **Generated Answer:** {answer}  \n\n        Assign a single integer score (1-5) based on the above criteria.\n        Only return the score as a number, without any extra text.\n\n        Verdict [1 | 2 | 3 | 4 | 5]: \n        \"\"\"\n    )\n\n    score = int(output.content.strip())\n    return score\n</code></pre>"},{"location":"reference/evaluation/metrics/rouge/","title":"ROUGE","text":"<p>ROUGE score metric for evaluating similarity between generated and reference answers.</p>"},{"location":"reference/evaluation/metrics/rouge/#ragbot.evaluation.metrics.rouge.ROUGE","title":"<code>ROUGE</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Metric</code></p> <p>ROUGE score metric for RAG evaluation.</p> <p>Uses <code>rouge_score</code> to compute overlap-based metrics such as ROUGE-1 or ROUGE-L. You can configure the specific ROUGE type and the score mode (F1, precision, recall).</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the metric.</p> <code>rouge_type</code> <code>str</code> <p>Type of ROUGE to use ('rouge1' or 'rougeL').</p> <code>mode</code> <code>str</code> <p>Score mode ('fmeasure', 'precision', or 'recall').</p> Source code in <code>ragbot\\evaluation\\metrics\\rouge.py</code> <pre><code>@dataclass\nclass ROUGE(Metric):\n    \"\"\"ROUGE score metric for RAG evaluation.\n\n    Uses `rouge_score` to compute overlap-based metrics such as ROUGE-1 or ROUGE-L.\n    You can configure the specific ROUGE type and the score mode (F1, precision, recall).\n\n    Attributes:\n        name (str): Name of the metric.\n        rouge_type (str): Type of ROUGE to use ('rouge1' or 'rougeL').\n        mode (str): Score mode ('fmeasure', 'precision', or 'recall').\n    \"\"\"\n\n    name: str = field(default=\"rouge\", repr=True)\n    _required_columns: Set[str] = field(\n        default_factory=lambda: {\"answer\", \"reference_answer\"}\n    )\n    rouge_type: Literal[\"rouge1\", \"rougeL\"] = \"rougeL\"\n    mode: Literal[\"fmeasure\", \"precision\", \"recall\"] = \"fmeasure\"\n\n    def __post_init__(self):\n        \"\"\"Initialize the ROUGE scorer and ensure required package is available.\"\"\"\n        try:\n            from rouge_score import rouge_scorer\n        except ImportError as e:\n            raise ImportError(\n                f\"{e.name} is required. Please install it with `pip install {e.name}`\"\n            )\n        self.rouge_scorer = rouge_scorer\n\n    def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n        \"\"\"Compute ROUGE score for a given sample.\n\n        Args:\n            sample (Sample): Sample containing both `answer` and `reference_answer`.\n            **kwargs: Optional keyword arguments (not used here).\n\n        Returns:\n            float: The ROUGE score (f-measure, precision, or recall depending on config).\n        \"\"\"\n        scorer = self.rouge_scorer.RougeScorer([self.rouge_type], use_stemmer=True)\n        scores = scorer.score(sample.reference_answer, sample.answer)\n        return getattr(scores[self.rouge_type], self.mode)\n</code></pre>"},{"location":"reference/evaluation/metrics/rouge/#ragbot.evaluation.metrics.rouge.ROUGE.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialize the ROUGE scorer and ensure required package is available.</p> Source code in <code>ragbot\\evaluation\\metrics\\rouge.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the ROUGE scorer and ensure required package is available.\"\"\"\n    try:\n        from rouge_score import rouge_scorer\n    except ImportError as e:\n        raise ImportError(\n            f\"{e.name} is required. Please install it with `pip install {e.name}`\"\n        )\n    self.rouge_scorer = rouge_scorer\n</code></pre>"},{"location":"reference/evaluation/metrics/rouge/#ragbot.evaluation.metrics.rouge.ROUGE.score","title":"<code>score(sample, **kwargs)</code>","text":"<p>Compute ROUGE score for a given sample.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Sample</code> <p>Sample containing both <code>answer</code> and <code>reference_answer</code>.</p> required <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments (not used here).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The ROUGE score (f-measure, precision, or recall depending on config).</p> Source code in <code>ragbot\\evaluation\\metrics\\rouge.py</code> <pre><code>def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n    \"\"\"Compute ROUGE score for a given sample.\n\n    Args:\n        sample (Sample): Sample containing both `answer` and `reference_answer`.\n        **kwargs: Optional keyword arguments (not used here).\n\n    Returns:\n        float: The ROUGE score (f-measure, precision, or recall depending on config).\n    \"\"\"\n    scorer = self.rouge_scorer.RougeScorer([self.rouge_type], use_stemmer=True)\n    scores = scorer.score(sample.reference_answer, sample.answer)\n    return getattr(scores[self.rouge_type], self.mode)\n</code></pre>"},{"location":"reference/evaluation/metrics/semantic_similarity/","title":"Semantic Similarity","text":"<p>Semantic similarity metric for evaluating the similarity between generated and reference answers.</p>"},{"location":"reference/evaluation/metrics/semantic_similarity/#ragbot.evaluation.metrics.semantic_similarity.SemanticSimilarity","title":"<code>SemanticSimilarity</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricWithEmbeddings</code></p> <p>Semantic similarity metric for RAG evaluation.</p> <p>This metric evaluates the semantic similarity between a generated answer and a reference answer by comparing their embeddings. The similarity is measured using cosine similarity between the two embedding vectors.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the metric.</p> Source code in <code>ragbot\\evaluation\\metrics\\semantic_similarity.py</code> <pre><code>@dataclass\nclass SemanticSimilarity(MetricWithEmbeddings):\n    \"\"\"Semantic similarity metric for RAG evaluation.\n\n    This metric evaluates the semantic similarity between a generated answer and a reference answer\n    by comparing their embeddings. The similarity is measured using cosine similarity between the\n    two embedding vectors.\n\n    Attributes:\n        name (str): The name of the metric.\n    \"\"\"\n\n    name: str = field(default=\"semantic similarity\", repr=True)\n    _required_columns: Set[str] = field(\n        default_factory=lambda: {\"answer\", \"reference_answer\"}\n    )\n\n    def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n        \"\"\"Compute the semantic similarity score between the generated answer and the reference answer.\n\n        This method evaluates the similarity between the answer and the reference answer by comparing\n        their embeddings. It computes the cosine similarity between the two vectors.\n\n        Args:\n            sample (Sample): A sample containing the generated answer and the reference answer.\n            **kwargs: Optional keyword arguments (not used here).\n\n        Returns:\n            float: A similarity score between 0 and 1 indicating the degree of similarity between the\n                  generated answer and the reference answer.\n        \"\"\"\n        reference, answer = sample.reference_answer, sample.answer\n\n        # Handle embeddings for empty strings\n        reference = reference or \" \"\n        answer = answer or \" \"\n\n        ref_embedding = np.array(self.embeddings.embed_query(reference))\n        ans_embedding = np.array(self.embeddings.embed_query(answer))\n\n        ref_norms = np.linalg.norm(ref_embedding, keepdims=True)\n        ans_norms = np.linalg.norm(ans_embedding, keepdims=True)\n        normalized_ref_emb = ref_embedding / ref_norms\n        normalized_ans_emb = ans_embedding / ans_norms\n        similarity = normalized_ref_emb @ normalized_ans_emb.T\n        score = similarity.flatten()\n        return score.tolist()[0]\n</code></pre>"},{"location":"reference/evaluation/metrics/semantic_similarity/#ragbot.evaluation.metrics.semantic_similarity.SemanticSimilarity.score","title":"<code>score(sample, **kwargs)</code>","text":"<p>Compute the semantic similarity score between the generated answer and the reference answer.</p> <p>This method evaluates the similarity between the answer and the reference answer by comparing their embeddings. It computes the cosine similarity between the two vectors.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Sample</code> <p>A sample containing the generated answer and the reference answer.</p> required <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments (not used here).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>A similarity score between 0 and 1 indicating the degree of similarity between the   generated answer and the reference answer.</p> Source code in <code>ragbot\\evaluation\\metrics\\semantic_similarity.py</code> <pre><code>def score(self, sample: Sample, **kwargs: Any) -&gt; float:\n    \"\"\"Compute the semantic similarity score between the generated answer and the reference answer.\n\n    This method evaluates the similarity between the answer and the reference answer by comparing\n    their embeddings. It computes the cosine similarity between the two vectors.\n\n    Args:\n        sample (Sample): A sample containing the generated answer and the reference answer.\n        **kwargs: Optional keyword arguments (not used here).\n\n    Returns:\n        float: A similarity score between 0 and 1 indicating the degree of similarity between the\n              generated answer and the reference answer.\n    \"\"\"\n    reference, answer = sample.reference_answer, sample.answer\n\n    # Handle embeddings for empty strings\n    reference = reference or \" \"\n    answer = answer or \" \"\n\n    ref_embedding = np.array(self.embeddings.embed_query(reference))\n    ans_embedding = np.array(self.embeddings.embed_query(answer))\n\n    ref_norms = np.linalg.norm(ref_embedding, keepdims=True)\n    ans_norms = np.linalg.norm(ans_embedding, keepdims=True)\n    normalized_ref_emb = ref_embedding / ref_norms\n    normalized_ans_emb = ans_embedding / ans_norms\n    similarity = normalized_ref_emb @ normalized_ans_emb.T\n    score = similarity.flatten()\n    return score.tolist()[0]\n</code></pre>"},{"location":"reference/utils/utils/","title":"Utils","text":"<p>Utilities for configuring and loading language models and embeddings.</p>"},{"location":"reference/utils/utils/#ragbot.utils.utils.get_embeddings","title":"<code>get_embeddings(provider, model_id)</code>","text":"<p>Load an embeddings model from the specified provider.</p> <p>Currently only supports Google Generative AI embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the embedding provider. Supported value: \"google\".</p> required <code>model_id</code> <code>str</code> <p>Identifier of the embedding model (e.g., \"models/embedding-001\").</p> required <p>Returns:</p> Type Description <code>Embeddings</code> <p>An instance of a LangChain-compatible embeddings model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provider is not recognized.</p> Source code in <code>ragbot\\utils\\utils.py</code> <pre><code>def get_embeddings(provider: str, model_id: str) -&gt; Embeddings:\n    \"\"\"Load an embeddings model from the specified provider.\n\n    Currently only supports Google Generative AI embeddings.\n\n    Args:\n        provider: Name of the embedding provider. Supported value: \"google\".\n        model_id: Identifier of the embedding model (e.g., \"models/embedding-001\").\n\n    Returns:\n        An instance of a LangChain-compatible embeddings model.\n\n    Raises:\n        ValueError: If the provider is not recognized.\n    \"\"\"\n    if provider == \"google\":\n        embeddings = GoogleGenerativeAIEmbeddings(model=model_id)\n    else:\n        raise ValueError(f\"Unknown provider: {provider}\")\n    return embeddings\n</code></pre>"},{"location":"reference/utils/utils/#ragbot.utils.utils.get_model","title":"<code>get_model(provider, model_id, temperature, top_p=0.85, top_k=40)</code>","text":"<p>Load a chat language model from the specified provider.</p> <p>Supports models from Google Generative AI, HuggingFace, and Ollama. Note that for Google Generative AI you will need to have the GOOGLE_API_KEY environment variable set, and for HuggingFace you will need the HUGGINGFACEHUB_API_TOKEN environment variable set.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the LLM provider. Supported values: \"google\", \"ollama\", \"hf\".</p> required <code>model_id</code> <code>str</code> <p>Model identifier or repository ID (e.g., \"gemini-1.5-flash\").</p> required <code>temperature</code> <code>float</code> <p>Sampling temperature to use (0.0 for deterministic output).</p> required <code>top_p</code> <code>float</code> <p>Nucleus sampling threshold. Only applicable to some providers.</p> <code>0.85</code> <code>top_k</code> <code>int</code> <p>Number of top tokens to consider. Only applicable to some providers.</p> <code>40</code> <p>Returns:</p> Type Description <code>BaseChatModel | LLM</code> <p>An instance of a language model (LLM or BaseChatModel).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provider is not recognized.</p> Source code in <code>ragbot\\utils\\utils.py</code> <pre><code>def get_model(\n    provider: str,\n    model_id: str,\n    temperature: float,\n    top_p: float = 0.85,\n    top_k: int = 40,\n) -&gt; BaseChatModel | LLM:\n    \"\"\"Load a chat language model from the specified provider.\n\n    Supports models from Google Generative AI, HuggingFace, and Ollama. Note that for Google\n    Generative AI you will need to have the GOOGLE_API_KEY environment variable set, and for\n    HuggingFace you will need the HUGGINGFACEHUB_API_TOKEN environment variable set.\n\n    Args:\n        provider: Name of the LLM provider. Supported values: \"google\", \"ollama\", \"hf\".\n        model_id: Model identifier or repository ID (e.g., \"gemini-1.5-flash\").\n        temperature: Sampling temperature to use (0.0 for deterministic output).\n        top_p: Nucleus sampling threshold. Only applicable to some providers.\n        top_k: Number of top tokens to consider. Only applicable to some providers.\n\n    Returns:\n        An instance of a language model (LLM or BaseChatModel).\n\n    Raises:\n        ValueError: If the provider is not recognized.\n    \"\"\"\n    if provider == \"google\":\n        # Configure Google AI Studio API key\n        genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n        llm = ChatGoogleGenerativeAI(\n            model=model_id, temperature=temperature, top_p=top_p, top_k=top_k\n        )\n    elif provider == \"ollama\":\n        llm = ChatOllama(model=model_id, temperature=temperature)\n    elif provider == \"hf\":\n        llm = HuggingFaceEndpoint(repo_id=model_id, temperature=temperature)\n    else:\n        raise ValueError(f\"Unknown provider: {provider}\")\n    return llm\n</code></pre>"},{"location":"reference/utils/utils/#ragbot.utils.utils.use_langsmith","title":"<code>use_langsmith(project_name)</code>","text":"<p>Configure LangSmith for experiment tracking.</p> <p>Sets environment variables to enable LangSmith's advanced tracing and logging features for the given project. Note that you must have the environment variable LANGCHAIN_API_KEY set to use this.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>Name of the LangSmith project to associate traces with.</p> required Source code in <code>ragbot\\utils\\utils.py</code> <pre><code>def use_langsmith(project_name: str):\n    \"\"\"Configure LangSmith for experiment tracking.\n\n    Sets environment variables to enable LangSmith's advanced tracing and logging features\n    for the given project. Note that you must have the environment variable LANGCHAIN_API_KEY\n    set to use this.\n\n    Args:\n        project_name: Name of the LangSmith project to associate traces with.\n    \"\"\"\n    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n    os.environ[\"LANGCHAIN_PROJECT\"] = project_name\n</code></pre>"}]}